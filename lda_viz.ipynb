{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the NPR articles with Latent Dirichlet Allocation\n",
    "\n",
    "  1. Run the LDA model with sklearn (http://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation)\n",
    "  2. Visualize it with pyldavis (https://pyldavis.readthedocs.io/en/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b0caa222f128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# USE Python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module='.*/IPython/.*')\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "df = pd.read_csv('npr_articles.csv', parse_dates=['date_published'])\n",
    "text = df['processed_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vectorize the words\n",
    "\n",
    "Essentially create a numeric representation of the words based on frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=max_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(text)\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = 6\n",
    "lda_model = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                      learning_method='online',\n",
    "                                      learning_offset=50.,\n",
    "                                      random_state=0)\n",
    "\n",
    "lda_model.fit(tf)\n",
    "pyLDAvis.sklearn.prepare(lda_model,tf, tf_vectorizer, R=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create a matrix of the top words used to define each topic\n",
    "top_words = 15\n",
    "tf_feature_names = tf_vectorizer.get_feature_names() \n",
    "print_top_words(lda_model,tf_feature_names,top_words)\n",
    "top_words = get_top_words(lda_model,tf_feature_names,top_words)\n",
    "all_top_words = set().union(*[v for v in top_words.values()])\n",
    "\n",
    "for key,vals in top_words.items():\n",
    "    print(key,\" \".join(vals))\n",
    "print(all_top_words)\n",
    "print(\"total words: %s\"%len(all_top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_words(model, feature_names, n_top_words):\n",
    "    top_words = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        _top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        top_words[str(topic_idx)] = _top_words\n",
    "    return(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_probas = np.zeros((max_features,n_topics),)\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    topic_probas[:,topic_idx] = topic\n",
    "print(topic_probas.shape)\n",
    "\n",
    "token_probas = lda_model.transform(tf)\n",
    "print(token_probas.shape)\n",
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_scatter(fit,ax,pcX=0,pcY=1,font_size=10,font_name='sans serif',ms=20,leg=True,title=None):\n",
    "    colors = ['k','cyan','r','orange','g','b','magenta']\n",
    "    #cvNames = np.sort(np.unique(covs[covariate]))\n",
    "    lines = []\n",
    "    #for _i,i in enumerate(cvNames):\n",
    "    #    indices = np.where(covs[covariate]==i)[0]\n",
    "    #    s = ax.scatter(fit[indices,pcX],fit[indices,pcY],c=colors[_i],s=ms,label=covariate,alpha=0.9)\n",
    "    #    lines.append(s)\n",
    "    indices = np.arange(fit.shape[0])\n",
    "    s = ax.scatter(fit[indices,pcX],fit[indices,pcY],s=ms,alpha=0.9)\n",
    "    lines.append(s)\n",
    "\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(font_size-2)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(font_size-2)\n",
    "\n",
    "    buff = 0.02\n",
    "    bufferX = buff * (fit[:,pcX].max() - fit[:,pcX].min())\n",
    "    bufferY = buff * (fit[:,pcY].max() - fit[:,pcY].min())\n",
    "    ax.set_xlim([fit[:,pcX].min()-bufferX,fit[:,pcX].max()+bufferX])\n",
    "    ax.set_ylim([fit[:,pcY].min()-bufferY,fit[:,pcY].max()+bufferY])\n",
    "    ax.set_xlabel(\"D-%s\"%str(pcX+1),fontsize=font_size,fontname=font_name)\n",
    "    ax.set_ylabel(\"D-%s\"%str(pcY+1),fontsize=font_size,fontname=font_name)\n",
    "    plt.locator_params(axis='x',nbins=5)\n",
    "    ax.set_aspect(1./ax.get_data_ratio())\n",
    "         \n",
    "    if title:\n",
    "        ax.set_title(title,fontsize=font_size+2,fontname=font_name)\n",
    "    #if leg:\n",
    "    #    legend = ax.legend(lines,cvNames,loc='upper right',scatterpoints=1,\n",
    "    #                       handletextpad=0.01,labelspacing=0.01,borderpad=0.1,handlelength=1.0)\n",
    "    #\n",
    "    #    for label in legend.get_texts():\n",
    "    #        label.set_fontsize(font_size-2)\n",
    "    #        label.set_fontname(font_name)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "mat = token_probas\n",
    "matScaled = preprocessing.scale(mat)\n",
    "pca_fit = PCA(n_components=2).fit_transform(mat)\n",
    "\n",
    "make_scatter(pca_fit,ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
